Metadata-Version: 2.4
Name: pred-fab
Version: 0.1.0
Summary: PFAB: A Predictive Layer for Digital Fabrication Systems
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy>=2.0.2
Requires-Dist: pandas>=2.3.1
Requires-Dist: scikit-learn>=1.6.1
Requires-Dist: scikit-optimize>=0.10.2
Provides-Extra: dev
Requires-Dist: pytest>=8.4.1; extra == "dev"
Dynamic: license-file

# Learning by Printing (LbP) Framework

A Python framework for iterative manufacturing process improvement through automated performance evaluation and optimization.

**Current package version**: 1.0.0  
**Architecture**: Phase 10 - Dimensional Prediction (November 2025)

## Table of Contents

- [Overview](#overview)
- [Framework Structure](#framework-structure)
- [Architecture Overview](#architecture-overview)
- [Installation](#installation)
  - [Prerequisites](#prerequisites)
  - [Install from Source](#install-from-source)
  - [Dependencies](#dependencies)
- [Quick Start](#quick-start)
- [Core Concepts](#core-concepts)
  - [Parameter Management System](#parameter-management-system)
  - [Dynamic Dimensionality System](#dynamic-dimensionality-system)
  - [Hierarchical Data Flow](#hierarchical-data-flow)
  - [Data Separation of Concerns](#data-separation-of-concerns)
- [API Reference](#api-reference)
  - [Interface APIs](#interface-apis)
    - [IExternalData](#iexternaldata-optional---databaseapi-integration)
    - [IFeatureModel](#ifeaturemodel-core---domain-specific-data-loading)
    - [IEvaluationModel](#ievaluationmodel-core---performance-assessment)
    - [IPredictionModel](#ipredictionmodel-optional---ml-workflows)
    - [ICalibrationStrategy](#icalibrationstrategy-optional---process-optimization)
  - [Orchestration APIs](#orchestration-apis)
    - [LBPAgent](#lbpagent-orchestration)
  - [Utility APIs](#utility-apis)
    - [Parameter Decorators](#parameter-decorators)
    - [Logging System](#logging-system)
    - [Local Data Operations](#local-data-operations)
- [Configuration](#configuration)
  - [Data Structure (Standalone Operation)](#data-structure-standalone-operation)
- [Key Features](#key-features)
- [License](#license)
- [Support](#support)

## Overview

Learning by Printing is an iterative manufacturing optimization approach that systematically improves printing processes through automated performance evaluation and parameter adjustment. The framework enables closed-loop learning where each experiment provides feedback for process refinement.

For detailed methodology, see: [An Introduction to Learning by Printing](https://mediatum.ub.tum.de/doc/1781543/1781543.pdf)

## Repository Structure

```
lbp_package/
├── interfaces/              # Abstract base classes defining contracts
│   ├── external_data.py     # Data source interface
│   ├── features.py          # Feature extraction interface  
│   ├── evaluation.py        # Performance evaluation interface
│   ├── prediction.py        # ML prediction interface
│   └── calibration.py       # Optimization interface
├── orchestration/           # Workflow management classes
│   ├── management.py        # LBPManager - main orchestrator
│   ├── evaluation.py        # EvaluationSystem
│   └── prediction.py        # PredictionSystem  
└── utils/                   # Supporting utilities
    ├── parameter_handler.py # Parameter management system
    ├── local_data.py        # File operations
    └── logger.py            # Logging utilities
```

## Architecture Overview

![LBP Framework Architecture](UML_diagram.png)

This diagram illustrates the interface-based architecture of the LBP framework, showing the relationships between core interfaces and orchestration components.

## Installation

### Prerequisites
- Python 3.9+
- Git

### Install from Source

```bash
# Clone repository
git clone <repository-url>
cd lbp-package

# Setup virtual environment and install dependencies
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv sync

# Or with pip
pip install -e .
```

### Dependencies

The framework has minimal core dependencies:
- `numpy` - Array operations and numerical computing
- `pandas` - Data structure handling for CSV export

**Note**: The [examples repository](https://github.com/your-org/lbp-examples) may have additional dependencies for visualization and domain-specific implementations, but the core framework is lightweight.

## Quick Start

```python
from lbp_package import LBPManager

# Initialize system (external data interface is optional)
manager = LBPManager(
    root_folder=".",
    local_folder="./data",
    log_folder="./logs"
    # external_data_interface=YourDataInterface()  # Optional
)

# Add your evaluation models (with weights for calibration)
manager.add_evaluation_model("performance_code", YourEvaluationModel, weight=1.0)

# Run workflows with optional flag overrides
manager.initialize_for_study("study_001")
manager.run_evaluation(
    "study_001",
    exp_nrs=[1, 2, 3], 
    debug_flag=True,  # Override default
    visualize_flag=True
)
```

For complete working examples, see the [LBP Examples Repository](https://github.com/your-org/lbp-examples).

## Core Concepts

### Parameter Management System
Elegant decorator-based system distinguishing three parameter types:

```python
@dataclass
class MyEvaluationModel(IEvaluationModel):
    # Study-level parameters (constant across all experiments)
    power_rating: float = study_parameter(50.0)
    max_speed: float = study_parameter(100.0)
    
    # Experiment-level parameters (vary between experiments)  
    layer_time: float = exp_parameter()
    temperature: float = exp_parameter(200.0)
    
    # Dimensional parameters (change during execution)
    layer_id: int = dim_parameter()
    segment_id: int = dim_parameter()
```

**Parameter Types:**
- **`@study_parameter`**: Study-wide constants shared across all experiments (numerical model configuration, equipment limits)
- **`@exp_parameter`**: Experiment-specific parameters that vary between experiments (numerical process settings, material properties)  
- **`@dim_parameter`**: Dynamic runtime parameters that change during execution to track current dimensional position

**Critical Parameter Requirements**:
- **Numerical types only**: Use `float` and `int` types for all parameters
- **No string parameters**: String parameters will cause errors in prediction and calibration workflows  
- **Default values**: Provide defaults for `@study_parameter` and `@exp_parameter` where possible
- **Type consistency**: Parameter types must match between record data and model declarations

**Automatic Parameter Access**: Once declared with decorators, parameters become automatically available as `self` attributes in all model methods.

### Dynamic Dimensionality System
Framework creates and manages multi-dimensional arrays automatically based on your dimension configuration:

**Concept**: Dimensionality defines the discrete evaluation structure - how your analysis is segmented (layers, segments, time steps, spatial regions, etc.). The framework dynamically creates and manages multi-dimensional arrays based on your dimension definitions.

**Key Benefits**:
- **Automatic Array Creation**: Framework creates feature and performance arrays sized `[n_layers, n_segments, ...]`
- **Dynamic Iteration**: Automatically loops through all dimensional combinations `(0,0), (0,1), (1,0), (1,1)`  
- **Database Alignment**: Dimension parameter names must match fields in your data records for dynamic configuration
- **Scalable**: Works with any number of dimensions - add new dimensions just by extending the lists

### Hierarchical Data Flow
Three-tier data management system providing complete database independence while enabling seamless integration:

**Data Loading Hierarchy** (Memory → Local → External):
1. **Memory First**: Check if data already loaded in current session
2. **Local Files**: Load from JSON/CSV files in local filesystem  
3. **External Source**: Query database/API only if data missing from local cache
4. **Automatic Caching**: Once loaded from external, automatically saved locally for future use

**Data Loading Control**:
- **`recompute_flag=False`**: Use hierarchical loading (memory → local → external)
- **`recompute_flag=True`**: Force loading from external source, bypassing local cache. Force overwritting of local files.

**Complete Database Independence**: Framework works fully without any external data interface. Manual data creation with JSON files supported. Add external data interface later without changing existing workflows.

### Data Separation of Concerns
Clear boundary between structured and unstructured data handling:

**Structured Data** (Handled by framework): Study/experiment records, feature arrays, performance results with standardized JSON/CSV format.

**Unstructured Data** (Complete user control): Domain-specific raw data (CAD files, sensor streams, images, proprietary formats) loaded via `FeatureModel._load_data()`.

## API Reference

The framework uses **interface-based design** with five core interfaces and supporting orchestration/utility classes.

### Interface APIs

#### IExternalData (Optional - Database/API Integration)

**Purpose**: Interface structured metadata access when integrating with databases/APIs. Framework works completely standalone without this interface.

**Required Methods:**

| Method | Description | Returns |
|--------|-------------|---------|
| `pull_study_record(study_code)` | Fetch study metadata from external source | Dict with id, Code, Parameters, Performance |
| `pull_exp_record(exp_code)` | Fetch experiment metadata from external source | Dict with id, Code, Parameters |

**Required Record Structure:**

Study records must contain these fields:
```python
{
    "id": int,                    # Unique study identifier
    "Code": str,                  # Study code (matches study_code parameter)
    "Parameters": Dict[str, Any], # Study-level parameters (@study_parameter values)
    "Performance": List[str]      # List of performance metric codes
}
```

Experiment records must contain these fields:
```python
{
    "id": int,                    # Unique experiment identifier  
    "Code": str,                  # Experiment code (matches exp_code parameter)
    "Parameters": Dict[str, Any]  # Experiment-level parameters (@exp_parameter values)
}
```

**Optional Methods** (default implementations provided):

| Method | Description | Returns |
|--------|-------------|---------|
| `push_study_records(study_codes, data, recompute)` | Save study records to external source | bool |
| `push_exp_records(exp_codes, data, recompute)` | Save experiment records to external source | bool |
| `pull_aggr_metrics(exp_codes)` | Fetch aggregated metrics from external source | Tuple[missing_codes, data_dict] |
| `push_aggr_metrics(exp_codes, data, recompute)` | Save aggregated metrics to external source | bool |

**Import**: `from lbp_package import IExternalData`

#### IFeatureModel (Core - Domain-Specific Feature Extraction)

**Purpose**: Extract numerical features from domain-specific raw data. You have complete control over data format and loading mechanism.

**Required Methods:**

| Method | Description | Returns |
|--------|-------------|---------|
| `_load_data(exp_code, exp_folder, debug_flag)` | Load raw data from experiment folder | Any (your custom data format) |
| `_compute_features(data, visualize_flag)` | Extract numerical features from loaded data | Dict[str, float] |

**Return Structure Requirements**:
- `_compute_features()` must return `Dict[str, float]` with string keys and numerical values
- Feature keys become available for use in `IPredictionModel.input` field
- All values must be finite numbers (no NaN, inf) - framework validates automatically

**Key Responsibility**: Load unstructured data (CAD files, sensor streams, proprietary formats) and convert to numerical features.

**Parameter Access**: Use `@study_parameter`, `@exp_parameter`, `@dim_parameter` decorators - automatically available as `self.attribute_name`.

**Import**: `from lbp_package import IFeatureModel`

#### IEvaluationModel (Core - Performance Assessment)

**Purpose**: Evaluate feature values against target values with automatic dimensionality handling.

**Required Properties:**

| Property | Description | Example |
|----------|-------------|---------|
| `dim_names` | Dimension names for arrays | `['layers', 'segments']` |
| `dim_param_names` | Parameter names in exp records | `['n_layers', 'n_segments']` |
| `dim_iterator_names` | Your @dim_parameter attribute names | `['layer_id', 'segment_id']` |
| `feature_model_type` | Associated feature model class | `ThermalFeatureModel` |
| `target_value` | Performance target for evaluation | `100.0` |

**Critical Naming Requirements**:
- `dim_param_names` must **exactly match** field names in experiment record `"Parameters"` section
- `dim_iterator_names` must **exactly match** `@dim_parameter` attribute names in your class
- Mismatch will cause runtime errors during dimension configuration

**Key Responsibility**: Framework automatically creates multi-dimensional arrays and iterates through all combinations based on your dimension configuration.

**Import**: `from lbp_package import IEvaluationModel`

#### IPredictionModel (Optional - ML Workflows)

**Purpose**: Train ML models to predict features from parameters, enabling performance forecasting.

**Required Properties:**

| Property | Description | Example |
|----------|-------------|---------|
| `feature_names` | List of feature names to predict | `['filament_width', 'layer_height']` |
| `required_features` | List of external features needed for prediction | `['temperature_measured']` |

**Required Methods:**

| Method | Description | Returns |
|--------|-------------|---------|
| `train(X, y, **kwargs)` | Train ML model on parameters (X) and features (y) | None |
| `forward_pass(X)` | Predict features for new parameters | pd.DataFrame |

**Training with Hyperparameters**:
The `**kwargs` parameter allows passing custom hyperparameters to your model:
```python
# User code
agent.train(
    datamodule,
    learning_rate=0.001,
    epochs=100,
    batch_size=32,
    verbose=True
)

# Your model implementation
def train(self, X: pd.DataFrame, y: pd.DataFrame, **kwargs):
    lr = kwargs.get('learning_rate', 0.01)
    epochs = kwargs.get('epochs', 50)
    # Implement training logic
```

**DataModule Integration**:
- Features (y) are automatically normalized by DataModule before training
- Predictions are automatically denormalized before returning to user
- User specifies normalization method: `'standard'`, `'minmax'`, `'robust'`, or `'none'`

**Import**: `from lbp_package import IPredictionModel`

#### ICalibrationModel (Optional - Process Optimization)

**Purpose**: Optimize process parameters using any optimization algorithm of your choice.

**Required Method:**

| Method | Description | Returns |
|--------|-------------|---------|
| `optimize(param_ranges, objective_fn)` | Find optimal parameters using your chosen algorithm | Dict[str, float] |

**Key Responsibility**: Implement optimization logic. Framework provides objective function based on evaluation model weights. You choose the optimization algorithm (scipy, scikit-optimize, genetic algorithms, etc.).

**Import**: `from lbp_package import ICalibrationModel`

### Orchestration APIs

#### LBPManager (Orchestration)

**Purpose**: Main orchestration class coordinating the complete learning workflow.

**Initialization:**
```python
from lbp_package import LBPManager

manager = LBPManager(
    root_folder=".", local_folder="./data", log_folder="./logs",
    external_data_interface=None,  # Optional
    debug_flag=False, recompute_flag=False, visualize_flag=True
)
```

**Model Registration:**

| Method | Description |
|--------|-------------|
| `add_evaluation_model(performance_code, model_class, weight=None, **kwargs)` | Register evaluation model for specific performance metric |
| `add_prediction_model(performance_codes, model_class, **kwargs)` | Register ML model for multiple performance metrics |
| `set_calibration_model(model_class, **kwargs)` | Register optimization model for parameter tuning |

**Custom Model Arguments:**
All registration methods accept `**kwargs` to pass additional arguments to your model constructors:
```python
# Pass custom arguments to your model instances
manager.add_evaluation_model("temperature", MyTempModel, weight=1.0, 
                           custom_param=42, tolerance=0.1)
manager.add_prediction_model(["temp", "pressure"], MyMLModel, 
                           learning_rate=0.01, n_estimators=100)
manager.set_calibration_model(MyOptimizerModel, algorithm="genetic", 
                            population_size=50)
```

**Registration Requirements**:
- **`performance_code`**: Must match codes listed in study record `"Performance"` array
- **`weight`**: Required for calibration workflow - defines relative importance in multi-objective optimization
- **`performance_codes`** (prediction): Must be subset of registered evaluation model codes
- **Calibration dependency**: Requires at least one evaluation model with `weight > 0`

**Workflow Execution:**

| Method | Description |
|--------|-------------|
| `initialize_for_study(study_code)` | Set up framework for specific study |
| `run_evaluation(study_code, exp_nrs, **flag_overrides)` | Execute evaluation workflow for experiments |
| `run_training(study_code, exp_nrs, **flag_overrides)` | Execute ML training workflow |
| `run_calibration(exp_nr, param_ranges)` | Execute optimization workflow |

**Flag System Requirements**:
- **`debug_flag`**: `True` = skip external data operations, `False` = use external data interface if available  
- **`recompute_flag`**: `True` = force recomputation and overwrite existing results, `False` = use cached results
- **`visualize_flag`**: `True` = enable visualizations during processing, `False` = skip visualizations
- **Flag precedence**: Method-level overrides > initialization defaults

**Parameter Range Requirements** (for calibration):
- Keys must match parameter names in experiment records
- Values must be `(min_value, max_value)` tuples with `min_value < max_value`
- Only numerical parameters supported - framework validates against `@exp_parameter` types

### Utility APIs

The framework provides utility classes for advanced users who need direct access to internal operations.

#### Parameter Decorators

**Access**: `from lbp_package.utils import study_parameter, exp_parameter, dim_parameter`

Use these decorators in your `@dataclass` interface implementations to mark parameter types. Parameter management is handled automatically by the framework.

#### Logging System

**Access**: `manager.logger` (LBPLogger instance)

Provides dual logging to file and console with automatic session files and ANSI code handling.

| Method | Description |
|--------|-------------|
| `debug(message)` | Log debug message to file only |
| `info(message)` | Log info message to file only |
| `warning(message)` | Log warning message to file only |
| `error(message)` | Log error message to file only |
| `console_info(message)` | Print to console and log as info |
| `console_success(message)` | Print success message with ✅ formatting |
| `console_warning(message)` | Print warning message with ⚠️ formatting |
| `console_summary(message)` | Print formatted summary (strips ANSI codes in log) |

#### Local Data Operations

**Access**: `manager.local_data` (LocalData instance)

Direct file system operations for custom data management workflows.

| Method | Description |
|--------|-------------|
| `set_study_code(study_code)` | Set study context (required for most operations) |
| `get_experiment_code(exp_nr)` | Generate experiment code from number |
| `get_experiment_folder(exp_code)` | Get full path to experiment folder |
| `get_server_experiment_folder(exp_code)` | Get server experiment folder path |
| `get_experiment_file_path(exp_code, filename)` | Get full path to file in experiment |
| `list_experiments()` | List all experiment folders in study |
| `copy_to_folder(src_path, target_folder)` | Copy file/folder to target directory |
| `check_folder_access(folder_path)` | Verify folder accessibility |
| `check_availability(code, memory)` | Check if code exists in memory dict |


## Configuration

### Data Structure (Standalone Operation)

If working without external data interface, create these JSON structures manually:

**Study Record** (`./data/thermal_study/study_record.json`):
```json
{
    "id": 1,
    "Code": "thermal_study", 
    "Parameters": {
        "target_temp": 200.0,
        "sampling_rate": 1000.0
    },
    "Performance": [
        "temperature_control"
    ]
}
```

**Experiment Record** (`./data/thermal_study/thermal_study_001/exp_record.json`):  
```json
{
    "id": 1,
    "Code": "thermal_study_001",
    "Parameters": {
        "n_layers": 5,
        "n_segments": 8,
        "print_speed": 50.0
    }
}
```

The framework automatically creates the folder structure and manages all result files.

## Key Features

- **Interface-Based Design**: Clean contracts for feature extraction, evaluation, prediction, and calibration
- **Type-Safe Data Model**: DataObject instances define schema/validation rules, DataBlock instances store actual values with clean separation to prevent data corruption
- **Database Independence**: Works standalone with JSON files or integrates with existing databases  
- **Hierarchical Data Flow**: Automatic caching (memory → local → external) with recompute control
- **Train/Val/Test Splits**: Built-in data splitting for proper ML evaluation with reproducible random seeds
- **Model Validation**: Automatic computation of MAE, RMSE, and R² metrics on validation/test sets
- **Dynamic Dimensionality**: Automatic multi-dimensional array management for complex analysis
- **Batched Prediction**: Memory-efficient processing with configurable batch sizes and overlap for context-aware models
- **Normalization**: Configurable feature normalization (standard, minmax, robust) fitted on training data
- **Declarative Rounding**: Configure rounding at the data definition level (e.g., `Parameter.real(round_digits=3)`)
- **Flag-Based Configuration**: Runtime control without configuration files - pure programmatic setup
- **Parameter Management**: Type-safe parameter system with validation and automatic rounding

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

<!-- ## Citation

If you use this framework in your research, please cite:

```bibtex
@article{learning_by_printing,
  title={An Introduction to Learning by Printing},
  author={Your Authors},
  journal={Your Journal},
  year={2024},
  url={https://mediatum.ub.tum.de/doc/1781543/1781543.pdf}
}
``` -->

## Support

- **Examples Repository**: [LBP Examples](https://gitlab.lrz.de/cms/dev/robotlab/learning-by-printing/lbp_package_examples) - Complete working implementations

